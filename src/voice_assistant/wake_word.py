"""æ™ºèƒ½è¯­éŸ³å”¤é†’ç³»ç»Ÿ - åŒé˜¶æ®µè¯†åˆ«ç‰ˆ"""
import time
import numpy as np
import pyaudio
import sherpa_onnx
from pathlib import Path

from .config import (
    MODELS_DIR,
    SAMPLE_RATE,
    CHUNK_SIZE,
    RECORD_SECONDS,
    SILENCE_THRESHOLD,
    MAX_SILENCE_FRAMES,
    DEFAULT_WAKE_WORDS,
    CONFIG_DIR,
)
from .llm import LLMController


class SmartWakeWordSystem:
    """æ™ºèƒ½è¯­éŸ³å”¤é†’ç³»ç»Ÿ - åŒé˜¶æ®µè¯†åˆ«ç‰ˆ"""

    def __init__(self, models_dir=None, enable_voice=True):
        self.models_dir = Path(models_dir) if models_dir else MODELS_DIR
        self.running = False
        self.sample_rate = SAMPLE_RATE
        self.enable_voice = enable_voice

        print("æ­£åœ¨åˆå§‹åŒ–æ™ºèƒ½è¯­éŸ³åŠ©æ‰‹...")

        # é˜¶æ®µ1: KWSæ¨¡å‹ï¼ˆè½»é‡çº§ï¼‰
        self.kws_model = self.create_kws_model()

        # é˜¶æ®µ2: ASRæ¨¡å‹ï¼ˆé‡é‡çº§ï¼‰
        self.asr_model = self.create_asr_model()

        # æ§åˆ¶å™¨
        self.controller = LLMController()

        print(f"âœ“ KWSæ¨¡å‹å·²åŠ è½½")
        print(f"âœ“ ASRæ¨¡å‹å·²åŠ è½½")
        print(f"è¯­éŸ³æ’­æŠ¥: {'å¼€å¯' if enable_voice else 'å…³é—­'}")

    def create_kws_model(self):
        """åˆ›å»ºKWSå…³é”®è¯æ£€æµ‹æ¨¡å‹"""
        kws_dir = self.models_dir / "sherpa-onnx-kws-zipformer-wenetspeech-3.3M-2024-01-01"

        if not kws_dir.exists():
            raise FileNotFoundError(f"KWSæ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {kws_dir}")

        # åˆ›å»ºå…³é”®è¯æ–‡ä»¶ï¼ˆæ ¼å¼ï¼šæ‹¼éŸ³éŸ³èŠ‚ @ä¸­æ–‡ï¼‰
        keywords_file = CONFIG_DIR / "keywords.txt"
        if not keywords_file.exists():
            print("âš ï¸  åˆ›å»ºé»˜è®¤å…³é”®è¯æ–‡ä»¶...")
            keywords_file.parent.mkdir(parents=True, exist_ok=True)
            with open(keywords_file, 'w', encoding='utf-8') as f:
                # æ ¼å¼ï¼šæ‹¼éŸ³éŸ³èŠ‚(ç©ºæ ¼åˆ†éš”) @ä¸­æ–‡
                # ä½¿ç”¨å¸¦å£°è°ƒçš„æ‹¼éŸ³éŸµæ¯ï¼Œç©ºæ ¼åˆ†éš”
                f.write("x iÇo zh Ã¬ @å°æ™º\n")
                f.write("n Ç h Ço zh Ã¹ sh Ç’u @ä½ å¥½åŠ©æ‰‹\n")
                f.write("zh Ã¬ n Ã©ng zh Ã¹ sh Ç’u @æ™ºèƒ½åŠ©æ‰‹\n")

        kws = sherpa_onnx.KeywordSpotter(
            tokens=str(kws_dir / "tokens.txt"),
            encoder=str(kws_dir / "encoder-epoch-12-avg-2-chunk-16-left-64.onnx"),
            decoder=str(kws_dir / "decoder-epoch-12-avg-2-chunk-16-left-64.onnx"),
            joiner=str(kws_dir / "joiner-epoch-12-avg-2-chunk-16-left-64.onnx"),
            num_threads=2,
            keywords_file=str(keywords_file),
            provider="cpu",
        )

        print(f"ğŸ“‹ åŠ è½½å…³é”®è¯: {keywords_file}")
        return kws

    def create_asr_model(self):
        """åˆ›å»ºASRå®Œæ•´è¯†åˆ«æ¨¡å‹"""
        model_file = self.models_dir / "sherpa-onnx-paraformer-zh-2024-03-09" / "model.int8.onnx"
        tokens_file = self.models_dir / "sherpa-onnx-paraformer-zh-2024-03-09" / "tokens.txt"

        if not model_file.exists():
            raise FileNotFoundError(f"ASRæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_file}")

        recognizer = sherpa_onnx.OfflineRecognizer.from_paraformer(
            str(model_file),
            str(tokens_file),
            num_threads=2,
            sample_rate=self.sample_rate,
            feature_dim=80,
            decoding_method="greedy_search",
            debug=False,
            provider="cpu"
        )
        return recognizer

    def start_listening(self):
        """å¼€å§‹ç›‘å¬"""
        print("\n" + "="*60)
        print("ğŸ¤ æ™ºèƒ½è¯­éŸ³åŠ©æ‰‹å·²å¯åŠ¨ - åŒé˜¶æ®µè¯†åˆ«æ¨¡å¼")
        print("é˜¶æ®µ1: æŒç»­ç›‘å¬å…³é”®è¯ï¼ˆè½»é‡çº§KWSï¼‰")
        print("é˜¶æ®µ2: å”¤é†’åå½•éŸ³è¯†åˆ«ï¼ˆé‡é‡çº§ASRï¼‰")
        print("æŒ‰ Ctrl+C åœæ­¢")
        print("="*60)

        self.running = True

        try:
            p = pyaudio.PyAudio()
            device_info = p.get_default_input_device_info()
            print(f"éº¦å…‹é£: {device_info['name']}")

            # æ‰“å¼€éŸ³é¢‘æµ
            stream = p.open(
                format=pyaudio.paFloat32,
                channels=1,
                rate=self.sample_rate,
                input=True,
                frames_per_buffer=CHUNK_SIZE
            )

            print("âœ“ å¼€å§‹ç›‘å¬å…³é”®è¯...\n")

            # åˆ›å»ºKWSæµ
            kws_stream = self.kws_model.create_stream()

            while self.running:
                # è¯»å–éŸ³é¢‘
                audio_bytes = stream.read(CHUNK_SIZE, exception_on_overflow=False)
                audio_data = np.frombuffer(audio_bytes, dtype=np.float32)

                # TTSæ’­æ”¾æœŸé—´ä»ç„¶ç›‘å¬ï¼Œä½†éœ€è¦æ›´é«˜éŸ³é‡æ‰è§¦å‘ï¼ˆå…è®¸æ‰“æ–­ï¼‰
                if self.controller.tts.is_playing:
                    # æ£€æµ‹éŸ³é‡å³°å€¼ï¼Œåˆ¤æ–­æ˜¯å¦æ˜¯çœŸå®çš„è¯­éŸ³æ‰“æ–­
                    volume = np.sqrt(np.mean(audio_data**2))
                    # å¦‚æœéŸ³é‡è¿‡ä½ï¼ˆå¯èƒ½æ˜¯TTSå›å£°ï¼‰ï¼Œè·³è¿‡æ£€æµ‹
                    if volume < 0.02:  # é™ä½é˜ˆå€¼ï¼Œæ›´å®¹æ˜“æ‰“æ–­
                        time.sleep(0.01)
                        continue
                    # éŸ³é‡è¶³å¤Ÿé«˜ï¼Œå¯èƒ½æ˜¯ç”¨æˆ·æ‰“æ–­ï¼Œç»§ç»­æ£€æµ‹

                # å–‚ç»™KWS
                kws_stream.accept_waveform(self.sample_rate, audio_data)

                # æ£€æµ‹å…³é”®è¯
                while self.kws_model.is_ready(kws_stream):
                    self.kws_model.decode_stream(kws_stream)

                # è·å–ç»“æœ
                result = self.kws_model.get_result(kws_stream)

                if result:
                    print(f"\nâœ¨ æ£€æµ‹åˆ°å”¤é†’è¯: {result}")

                    # æ‰“æ–­æ­£åœ¨æ’­æ”¾çš„TTS
                    if self.controller.tts.is_playing:
                        self.controller.tts.stop()

                    # æç¤ºéŸ³
                    try:
                        import winsound
                        winsound.Beep(800, 200)
                    except:
                        pass

                    if self.enable_voice:
                        self.controller.tts.speak_async("æˆ‘åœ¨")

                    # è¿›å…¥é˜¶æ®µ2
                    self._enter_command_mode(p)

                    # é‡ç½®KWSæµ
                    kws_stream = self.kws_model.create_stream()

            stream.stop_stream()
            stream.close()
            p.terminate()

        except KeyboardInterrupt:
            print("\nåœæ­¢ä¸­...")
        finally:
            self.running = False

    def _enter_command_mode(self, pyaudio_instance):
        """é˜¶æ®µ2: å½•éŸ³è¯†åˆ«"""
        print("ğŸ’¬ è¯·è¯´å‡ºæŒ‡ä»¤...")

        # ç­‰å¾…TTSæ’­å®Œ
        while self.controller.tts.is_playing:
            time.sleep(0.1)

        # å½•éŸ³å‚æ•°
        audio_buffer = []

        try:
            stream = pyaudio_instance.open(
                format=pyaudio.paFloat32,
                channels=1,
                rate=self.sample_rate,
                input=True,
                frames_per_buffer=1024
            )

            print("ğŸ™ï¸ å½•éŸ³ä¸­...")
            silence_count = 0

            for i in range(0, int(self.sample_rate / 1024 * RECORD_SECONDS)):
                audio_bytes = stream.read(1024, exception_on_overflow=False)
                audio_data = np.frombuffer(audio_bytes, dtype=np.float32)
                audio_buffer.append(audio_data)

                # é™éŸ³æ£€æµ‹
                volume = np.sqrt(np.mean(audio_data**2))
                if volume < SILENCE_THRESHOLD:
                    silence_count += 1
                    if silence_count > MAX_SILENCE_FRAMES and len(audio_buffer) > 10:
                        print("   æ£€æµ‹åˆ°é™éŸ³ï¼Œåœæ­¢å½•éŸ³")
                        break
                else:
                    silence_count = 0

            stream.stop_stream()
            stream.close()

            # æ‹¼æ¥éŸ³é¢‘
            full_audio = np.concatenate(audio_buffer)

            # ASRè¯†åˆ«
            print("ğŸ¤” æ­£åœ¨è¯†åˆ«...")
            asr_stream = self.asr_model.create_stream()
            asr_stream.accept_waveform(self.sample_rate, full_audio)
            self.asr_model.decode_stream(asr_stream)
            text = asr_stream.result.text.strip()

            if text:
                print(f"ğŸ“ è¯†åˆ«ç»“æœ: {text}")
                self._execute_command(text)
            else:
                print("   æœªè¯†åˆ«åˆ°å†…å®¹")
                if self.enable_voice:
                    self.controller.tts.speak_async("æŠ±æ­‰ï¼Œæˆ‘æ²¡å¬æ¸…")

        except Exception as e:
            print(f"å½•éŸ³è¯†åˆ«é”™è¯¯: {e}")

    def _execute_command(self, text):
        """æ‰§è¡Œå‘½ä»¤"""
        print(f"ğŸ¤– æ­£åœ¨ç†è§£æŒ‡ä»¤: {text}")

        intent = self.controller.understand_intent(text)
        print(f"ğŸ’¡ æ„å›¾: {intent}")

        self.controller.execute_action(intent, enable_voice=self.enable_voice)
        print("âœ“ æ‰§è¡Œå®Œæˆ\n")
