"""å¿«é€Ÿæµ‹è¯•å„ä¸ªæ¨¡å‹æ˜¯å¦å¯ç”¨"""
import sherpa_onnx
from pathlib import Path
import wave
import numpy as np

print("="*60)
print("ğŸ”Š æµ‹è¯• TTS (è¯­éŸ³åˆæˆ)")
print("="*60)

# TTSæµ‹è¯•
tts_dir = Path("models/vits-melo-tts-zh_en")
config = sherpa_onnx.OfflineTtsConfig(
    model=sherpa_onnx.OfflineTtsModelConfig(
        vits=sherpa_onnx.OfflineTtsVitsModelConfig(
            model=str(tts_dir / "model.onnx"),
            tokens=str(tts_dir / "tokens.txt"),
            data_dir=str(tts_dir / "espeak-ng-data"),
        )
    ),
    max_num_sentences=1,
)

tts = sherpa_onnx.OfflineTts(config)
print(f"âœ… TTSåŠ è½½æˆåŠŸï¼Œé‡‡æ ·ç‡: {tts.sample_rate} Hz")

# ç”ŸæˆéŸ³é¢‘ï¼ˆä½¿ç”¨sidè€Œä¸æ˜¯speaker_idï¼‰
text = "ä½ å¥½ï¼Œè¿™æ˜¯æµ‹è¯•"
print(f"ğŸ¤ ç”Ÿæˆæ–‡æœ¬: {text}")
audio = tts.generate(text, sid=0, speed=1.0)

# ä¿å­˜éŸ³é¢‘
output_file = "test_output.wav"
with wave.open(output_file, 'wb') as wf:
    wf.setnchannels(1)
    wf.setsampwidth(2)
    wf.setframerate(tts.sample_rate)
    wf.writeframes((audio.samples * 32767).astype(np.int16).tobytes())

print(f"âœ… å·²ç”Ÿæˆ: {output_file}")

# æ’­æ”¾
try:
    import winsound
    print("ğŸ”Š æ­£åœ¨æ’­æ”¾...")
    winsound.PlaySound(output_file, winsound.SND_FILENAME)
    print("âœ… æ’­æ”¾å®Œæˆ")
except:
    print("âš ï¸ æ— æ³•æ’­æ”¾ï¼Œä½†æ–‡ä»¶å·²ç”Ÿæˆ")

print("\n" + "="*60)
print("ğŸ™ï¸ æµ‹è¯• STT (è¯­éŸ³è¯†åˆ«)")
print("="*60)

# STTæµ‹è¯•
stt_dir = Path("models/sherpa-onnx-paraformer-zh-2024-03-09")
encoder = list(stt_dir.glob("*encoder*.onnx"))[0]

config = sherpa_onnx.OfflineRecognizerConfig(
    model_config=sherpa_onnx.OfflineModelConfig(
        paraformer=sherpa_onnx.OfflineParaformerModelConfig(
            model=str(encoder),
        ),
        tokens=str(stt_dir / "tokens.txt"),
        num_threads=2,
    )
)

recognizer = sherpa_onnx.OfflineRecognizer(config)
print(f"âœ… STTåŠ è½½æˆåŠŸï¼Œé‡‡æ ·ç‡: {recognizer.sample_rate} Hz")

# è¯†åˆ«åˆšæ‰ç”Ÿæˆçš„éŸ³é¢‘
print(f"ğŸ§ è¯†åˆ«éŸ³é¢‘: {output_file}")
with wave.open(output_file, 'rb') as wf:
    samples = np.frombuffer(wf.readframes(wf.getnframes()), dtype=np.int16)
    samples = samples.astype(np.float32) / 32768.0

stream = recognizer.create_stream()
stream.accept_waveform(tts.sample_rate, samples)
recognizer.decode_stream(stream)

result = stream.result.text
print(f"ğŸ“ è¯†åˆ«ç»“æœ: {result}")

print("\n" + "="*60)
print("ğŸšï¸ æµ‹è¯• VAD (è¯­éŸ³æ´»åŠ¨æ£€æµ‹)")
print("="*60)

vad_file = Path("models/silero_vad.onnx")
config = sherpa_onnx.VadModelConfig()
config.silero_vad.model = str(vad_file)
config.sample_rate = 16000

vad = sherpa_onnx.VoiceActivityDetector(config, buffer_size_in_seconds=10)
print(f"âœ… VADåŠ è½½æˆåŠŸ")

print("\n" + "="*60)
print("ğŸ‰ æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•é€šè¿‡ï¼")
print("="*60)
